# -*- coding: utf-8 -*-
"""nettoyage_for 3 models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15ktEAU3gGhD9HxxVA4YxqQJu6-3I8GW1
"""

import pandas as pd
import re
import spacy
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from transformers import BertTokenizer

nlp = spacy.load("fr_core_news_sm")
df = pd.read_csv("file.csv", encoding='utf-8')

def clean_common(text):
    if not isinstance(text, str):
        return ""
    text = re.sub(r'<[^>]+>|http\S+|www\.\S+|@\w+|#\w+', '', text)
    text = re.sub(r'[^\w\s!?.,]', ' ', text)
    return re.sub(r'\s+', ' ', text).strip()

df['text_clean'] = df['commentaire'].apply(clean_common)

def process_lstm_spacy(text):
    if not text:
        return ""
    text = text.lower()
    text = re.sub(r'(\w)\1{2,}', r'\1', text)
    doc = nlp(text)
    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and not token.is_space and token.text.strip() and len(token.text) > 1]
    return ' '.join(tokens)

df['text_lstm'] = df['text_clean'].apply(process_lstm_spacy)

tokenizer_lstm = Tokenizer(num_words=10000, oov_token="<OOV>")
tokenizer_lstm.fit_on_texts(df['text_lstm'])
sequences = tokenizer_lstm.texts_to_sequences(df['text_lstm'])
df['sequences_lstm'] = list(pad_sequences(sequences, maxlen=100, padding='post', truncating='post'))

def process_fasttext_spacy(text):
    if not text:
        return ""
    text = text.lower()
    text = re.sub(r'(\w)\1{2,}', r'\1', text)
    doc = nlp(text)
    tokens = [token.text for token in doc if not token.is_punct and not token.is_space and token.text.strip()]
    return ' '.join(tokens)

with open("fasttext_data_spacy.txt", "w", encoding="utf-8") as f:
    for text, label in zip(df['text_clean'], df['sentiment']):
        f.write(f"__label__{label} {process_fasttext_spacy(text)}\n")

def process_bert_spacy(text):
    if not isinstance(text, str) or not text:
        return ""
    text = re.sub(r'[^\w\s!?.,]', ' ', text)
    text = re.sub(r'(\w)\1{3,}', r'\1\1', text)
    doc = nlp(text)
    normalized_tokens = []
    for token in doc:
        if token.is_space:
            continue
        if token.is_punct and token.text in '!?.,:;':
            normalized_tokens.append(token.text)
        elif not token.is_punct:
            normalized_tokens.append(token.text)
    return ' '.join(normalized_tokens)

df['text_bert'] = df['commentaire'].apply(process_bert_spacy)
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')

bert_encodings = bert_tokenizer(
    df['text_bert'].tolist(),
    max_length=128,
    padding='max_length',
    truncation=True,
    return_tensors='pt'
)

df['bert_input_ids'] = [bert_encodings['input_ids'][i] for i in range(len(df))]
df['bert_attention_mask'] = [bert_encodings['attention_mask'][i] for i in range(len(df))]

output_df = df[['text_lstm', 'sequences_lstm', 'text_bert', 'bert_input_ids', 'bert_attention_mask', 'sentiment', 'probleme']].copy()
output_df['sequences_lstm'] = output_df['sequences_lstm'].apply(lambda x: ','.join(map(str, x)))
output_df['bert_input_ids'] = output_df['bert_input_ids'].apply(lambda x: ','.join(map(str, x.numpy().flatten())))
output_df['bert_attention_mask'] = output_df['bert_attention_mask'].apply(lambda x: ','.join(map(str, x.numpy().flatten())))
output_df.to_csv("processed_data_spacy.csv", index=False)

print(f"Traitement terminé: {len(df)} échantillons")